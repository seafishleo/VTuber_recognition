{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1KYSqpcRBJ6BulSbu4bV4ZaXKmbfZSml7","authorship_tag":"ABX9TyMf5clwALvXv2jmWT4r/Scc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5i2XpchWRsY4"},"outputs":[],"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import tensorflow as tf\n","\n","\n","def weight_variable(shape, name):\n","  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n","  initial = tf.truncated_normal(shape, stddev=0.1)\n","  return tf.Variable(initial, name=name)\n","\n","\n","def bias_variable(shape, name):\n","  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n","  initial = tf.constant(0.1, shape=shape)\n","  return tf.Variable(initial, name=name)\n","\n","\n","def conv2d(x, receptive_field, channels, name):\n","  kernel_shape = receptive_field + channels\n","  bias_shape = [channels[-1]]\n","\n","  W = weight_variable(kernel_shape, name+'-W')\n","  b = bias_variable(bias_shape, name+'-b')\n","\n","  conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n","  conv_bias = tf.nn.bias_add(conv, b)\n","\n","  return tf.nn.relu(conv_bias)\n","\n","def inference(input, batch_size, is_training=True):\n","\n","  # 3x128x128\n","  print(\"input \", input.shape)\n","  with tf.name_scope('conv1'): # conv 5x5\n","      h_conv1 = conv2d(input, [5, 5], [3, 16], 'conv1')\n","\n","      print(\"h_conv1  \", h_conv1.shape)\n","\n","\n","  # 16x128x128\n","  with tf.name_scope('pool1'):  # pooling 3x3\n","      h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n","      print(\"h_pool1  \", h_pool1.shape)\n","\n","\n","  # 16x64x64\n","  with tf.name_scope('conv2'): # conv 5x5\n","      h_conv2 = conv2d(h_pool1, [5, 5], [16, 32], 'conv2')\n","      print(\"h_conv2  \", h_conv2.shape)\n","\n","\n","  # 32x64x64\n","  with tf.name_scope('pool2'):  # pooling 3x3\n","      h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n","      print(\"h_pool2  \", h_pool2.shape)\n","\n","\n","  # 32x32x32\n","  with tf.name_scope('conv3'): # conv 3x3\n","      h_conv3 = conv2d(h_pool2, [3, 3], [32, 64], 'conv3')\n","      print(\"h_conv3  \", h_conv3.shape)\n","\n","\n","  # 64x32x32\n","  with tf.name_scope('pool3'):  # pooling 3x3\n","      h_pool3 = tf.nn.max_pool(h_conv3, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n","      print(\"h_pool3  \", h_pool3.shape)\n","\n","\n","  # 64x16x16\n","  with tf.name_scope('conv4'): # conv 3x3\n","      h_conv4 = conv2d(h_pool3, [3, 3], [64, 128], 'conv4')\n","      print(\"h_conv4  \", h_conv4.shape)\n","\n","\n","  # 128x16x16\n","  with tf.name_scope('pool4'): # pooling 3x3\n","      h_pool4 = tf.nn.max_pool(h_conv4, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n","      print(\"h_pool4  \", h_pool4.shape)\n","\n","\n","  # 128x8x8 = 8192\n","  with tf.name_scope('fc1'):\n","      h_pool4_flat = tf.reshape(h_pool4, [batch_size, -1])\n","      dim = h_pool4_flat.get_shape()[1].value\n","\n","      W_fc1 = weight_variable([dim, 1024], 'fc1-W')\n","      b_fc1 = bias_variable([1024], 'fc1-b')\n","\n","      h_fc1 = tf.nn.relu(tf.matmul(h_pool4_flat, W_fc1) + b_fc1)\n","      print(\"h_fc1    \", h_fc1.shape)\n","\n","  with tf.name_scope('dropout1'):\n","      h_fc1_drop = tf.layers.dropout(h_fc1, rate=0.2, training=is_training)\n","\n","  # 1024\n","\n","  with tf.name_scope('fc2'):\n","      W_fc2 = weight_variable([1024, 5], 'fc2-W')\n","      b_fc2 = bias_variable([5], 'fc2-b')\n","\n","      h_fc2 = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n","      print(\"h_fc2    \", h_fc2.shape)\n","\n","  return h_fc2"]}]}