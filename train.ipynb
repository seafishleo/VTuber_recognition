{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"16_wr00sRC_2bo0ldbFxuSNliRt_kY7DG","authorship_tag":"ABX9TyMNwycOy9ZSe8aST/y0lZV6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sXYqB8ZjUsis"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import time\n","import model\n","from tensorflow import gfile\n","from tensorflow import logging\n","from datetime import datetime\n","\n","BATCH_SIZE = 100\n","NUM_EPOCHS = 10\n","LEARNING_RATE = 0.001\n","LOGDIR = '/content/drive/MyDrive/VTuber_recognition/logdir/'\n","\n","IMAGE_WIDTH = 128\n","IMAGE_HEIGHT = 128\n","IMAGE_CHANNE = 3\n","TARGET_SIZE = 5\n","\n","INPUT_TRAIN_TFRECORD = '/content/drive/MyDrive/VTuber_recognition/train_tfrecords/*.tfrecords'\n","\n","def read_and_decode(filename_queue):\n","  reader = tf.TFRecordReader()\n","  key, value = reader.read(filename_queue)\n","\n","  features = tf.parse_single_example(\n","      value,\n","      features={'label'  : tf.FixedLenFeature([], tf.int64, default_value=0),\n","                'image'  : tf.FixedLenFeature([], tf.string, default_value=\"\"),\n","                'height' : tf.FixedLenFeature([], tf.int64, default_value=0),\n","                'width'  : tf.FixedLenFeature([], tf.int64, default_value=0),\n","                'dim'    : tf.FixedLenFeature([], tf.int64, default_value=0)\n","      })\n","\n","  label = tf.cast(features['label'], tf.int32)\n","  label = tf.one_hot(label, TARGET_SIZE)\n","\n","  height = tf.cast(features['height'], tf.int32)\n","  width = tf.cast(features['width'], tf.int32)\n","  dim = tf.cast(features['dim'], tf.int32)\n","\n","  image = tf.decode_raw(features['image'], tf.uint8)\n","  image = tf.cast(image, tf.float32)\n","  image = image / 255  # 0~1\n","  image = tf.reshape(image, [IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNE])\n","\n","  return image, label\n","\n","\n","def inputs(batch_size, num_epochs, input_tfrecord):\n","  if not num_epochs:\n","      num_epochs = None\n","\n","  with tf.name_scope('input'):\n","\n","      files = gfile.Glob(input_tfrecord)\n","      files = sorted(files)\n","\n","      print(\"files num : \", len(files))\n","\n","      if not files:\n","          raise IOError(\"Unable to find training files. data_pattern='\" +\n","                        input_tfrecord + \"'.\")\n","      logging.info(\"Number of training files: %s.\", str(len(files)))\n","\n","      filename_queue = tf.train.string_input_producer(files,\n","                                                      num_epochs=num_epochs,\n","                                                      shuffle=True)\n","\n","      image, label = read_and_decode(filename_queue)\n","\n","      print(\"image      :\", image.shape)\n","      print(\"label      :\", label.shape)\n","\n","      image_batch, label_batch = tf.train.shuffle_batch(\n","          [image, label],\n","          batch_size=batch_size,\n","          num_threads=10,\n","          capacity=10000 + 15 * batch_size,\n","          min_after_dequeue=10000,\n","          allow_smaller_final_batch=False # True --> error ...\n","          )\n","\n","      tf.summary.image('input', image_batch)\n","\n","      return image_batch, label_batch\n","\n","\n","if __name__ == \"__main__\":\n","\n","  with tf.Graph().as_default():\n","\n","    print('Reading batches...')\n","    image_batch, label_batch = inputs(batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, input_tfrecord=INPUT_TRAIN_TFRECORD)\n","\n","    print('build models...')\n","    y_conv = model.inference(image_batch, BATCH_SIZE, is_training=True)\n","\n","    with tf.name_scope('train'):\n","        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=label_batch))\n","        tf.summary.scalar('loss', loss)\n","\n","    global_step = tf.Variable(0, trainable=False)\n","    k = 100 * 10**3 # 100k steps\n","    learning_rate = tf.train.inverse_time_decay(LEARNING_RATE, global_step, k, 1, staircase=True)\n","\n","    train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n","\n","    # calculate accuracy\n","    with tf.name_scope('test'):\n","        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(label_batch, 1))\n","        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","        tf.summary.scalar('accuracy', accuracy)\n","\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","\n","    sv = tf.train.Supervisor(logdir=LOGDIR, global_step=global_step, save_summaries_secs=10, save_model_secs=120)\n","\n","    with sv.managed_session(config=config) as sess:\n","        print('start loop...' + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","\n","        try:\n","            step = 0\n","            while not sv.should_stop():\n","                start_time = time.time()\n","\n","                _, loss_value, g_step = sess.run([train_step, loss, global_step])\n","\n","                duration = time.time() - start_time\n","\n","                print('Step train %04d     : loss = %07.4f (%02.3f sec)' % (g_step,\n","                                                                          loss_value,\n","                                                                          duration))\n","\n","                if step % 100 == 0:\n","                    est_accuracy, est_y, gt_y = sess.run([accuracy, y_conv, label_batch])\n","                    print(\"Accuracy (for test data): {:5.2f}\".format(est_accuracy))\n","                    print(\"True Label:\", np.argmax(gt_y[0:15,], 1))\n","                    print(\"Est Label :\", np.argmax(est_y[0:15, ], 1))\n","\n","                step += 1\n","\n","        except tf.errors.OutOfRangeError:\n","            print('Done training for %d epochs, %d steps.' % (NUM_EPOCHS, step))\n","\n","        sv.Stop()\n","\n","    print('End loop...' + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"]}]}